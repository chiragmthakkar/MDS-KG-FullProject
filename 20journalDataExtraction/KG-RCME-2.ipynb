{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RCME - 15099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas.io.html import read_html\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df.to_csv('blankFile.csv')\n",
    "\n",
    "columnNames = ['searchTerm1','searchTerm2','url','Title']\n",
    "\n",
    "df = pd.read_csv(\"blankFile.csv\", names= columnNames , header = None)\n",
    "df = df.iloc[1:]\n",
    "\n",
    "for i in range(20):\n",
    "    section = \"section\" + str(i)\n",
    "    sectionData = \"sectionData\" + str(i)\n",
    "    df[section] = ''\n",
    "    df[sectionData] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "3\n",
      "In page:  0\n",
      "Searching for query:  https://www.tandfonline.com/action/doSearch?AllField=BIM+AND+Green+Building&SeriesKey=rcme20\n",
      "row:  1\n",
      "row:  2\n",
      "row:  3\n",
      "row:  4\n",
      "row:  5\n",
      "row:  6\n",
      "row:  7\n",
      "row:  8\n",
      "row:  9\n",
      "row:  10\n",
      "26\n",
      "3\n",
      "In page:  0\n",
      "Searching for query:  https://www.tandfonline.com/action/doSearch?AllField=BIM+AND+Green+Buildings&SeriesKey=rcme20\n",
      "row:  11\n",
      "row:  12\n",
      "row:  13\n",
      "row:  14\n",
      "row:  15\n",
      "row:  16\n",
      "row:  17\n",
      "row:  18\n",
      "row:  19\n",
      "row:  20\n",
      "315\n",
      "32\n",
      "In page:  0\n",
      "Searching for query:  https://www.tandfonline.com/action/doSearch?AllField=Building+Information+Modelling+AND+Green+Building&SeriesKey=rcme20\n",
      "row:  21\n",
      "row:  22\n",
      "row:  23\n",
      "row:  24\n",
      "row:  25\n",
      "row:  26\n",
      "row:  27\n",
      "row:  28\n",
      "row:  29\n",
      "row:  30\n",
      "315\n",
      "32\n",
      "In page:  0\n",
      "Searching for query:  https://www.tandfonline.com/action/doSearch?AllField=Building+Information+Modelling+AND+Green+Buildings&SeriesKey=rcme20\n",
      "row:  31\n",
      "row:  32\n",
      "row:  33\n",
      "row:  34\n",
      "row:  35\n",
      "row:  36\n",
      "row:  37\n",
      "row:  38\n",
      "row:  39\n",
      "row:  40\n"
     ]
    }
   ],
   "source": [
    "searchTerm1 = ['BIM', 'Building Information Modelling']\n",
    "searchTerm2 = ['Green Building', 'Green Buildings']\n",
    "\n",
    "\n",
    "# searchTerm1 = ['BIM' , 'Building Information Modelling','Building information model','6D BIM', 'Digital construction','Smart construction','Digital engineering']\n",
    "# searchTerm2 = ['Green Building' , 'Green Buildings','Sustainable building','Sustainable buildings','Environmentally friendly design' , 'Sustainability' , 'Sustainable development' , 'Energy','Water' ,'Waste' , 'Thermal comfort','Electricity' ,'Solar','Wind power' , 'Green infrastructure','Life cycle assessment']\n",
    "\n",
    "counter = 0\n",
    "tc = 0\n",
    "programSearchTerms1 = []\n",
    "programSearchTerms2 = []\n",
    "\n",
    "for term2 in searchTerm2:\n",
    "    text2 = \"\"\n",
    "    terms2 = term2.split(' ')\n",
    "    for t2 in terms2:\n",
    "        text2 = text2 + t2 + \"+\"\n",
    "\n",
    "    fileIndex = text2.rfind(\"+\")\n",
    "    te = text2[:fileIndex]\n",
    "    programSearchTerms2.append(te)\n",
    "\n",
    "for term1 in searchTerm1:\n",
    "    text1 = \"\"\n",
    "    terms1 = term1.split(' ')\n",
    "    for t2 in terms1:\n",
    "        text1 = text1 + t2 + \"+\"\n",
    "\n",
    "    fileIndex = text1.rfind(\"+\")\n",
    "    te = text1[:fileIndex]\n",
    "    programSearchTerms1.append(te)\n",
    "\n",
    "\n",
    "# https://www.tandfonline.com/action/doSearch?AllField=BIM+and+Green+Building&SeriesKey=rcme20\n",
    "    \n",
    "baseUrl = 'https://www.tandfonline.com/action/doSearch?AllField='\n",
    "querySet = []\n",
    "row = 0\n",
    "for item1 in programSearchTerms1:\n",
    "    for item2 in programSearchTerms2:\n",
    "        \n",
    "        query = baseUrl + item1 +\"+AND+\"+ item2+\"&SeriesKey=rcme20\"\n",
    "        # print(query)\n",
    "\n",
    "        response = requests.get(query)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        allParas = soup.find_all(\"p\")\n",
    "        dd = allParas[0].text\n",
    "        miniData = dd.split(' ')\n",
    "        NumberOfResultsStr = miniData[2]\n",
    "        indexOfcomma = NumberOfResultsStr.find(',')\n",
    "\n",
    "        if indexOfcomma != -1:\n",
    "            NumberOfResultsStr = NumberOfResultsStr.replace(',','')\n",
    "\n",
    "        number = int(NumberOfResultsStr)\n",
    "        print(number)\n",
    "        tc = tc + number\n",
    "        page = number // 10\n",
    "        lastPage = number % 10\n",
    "        if lastPage > 0:\n",
    "            page = page + 1\n",
    "\n",
    "        pageSize = page\n",
    "        print(pageSize)\n",
    "\n",
    "        cnt = 0\n",
    "        allLinks = []\n",
    "        \n",
    "        # for testing purpose only\n",
    "        pageSize =1 \n",
    "        for currentPage in range(0,pageSize):\n",
    "            print(\"In page: \",currentPage)\n",
    "            \n",
    "            if currentPage != 0:\n",
    "                newQuery = query + '&pageSize=10&subjectTitle=&startPage=' + str(currentPage)\n",
    "                cnt = cnt + 1\n",
    "            else:\n",
    "                newQuery = query\n",
    "            \n",
    "            print(\"Searching for query: \",newQuery)\n",
    "            response = requests.get(newQuery)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            mydivs = soup.findAll(\"a\", {'class':['ref nowrap']})\n",
    "            \n",
    "            for d in mydivs:\n",
    "                url = 'https://www.tandfonline.com' + d.get('href')\n",
    "                allLinks.append(url)\n",
    "\n",
    "        for link in allLinks:\n",
    "            df.loc[row,'searchTerm1'] = item1\n",
    "            df.loc[row,'searchTerm2'] = item2\n",
    "            df.loc[row,'url'] = link\n",
    "            html_content = requests.get(link).text\n",
    "            soup = BeautifulSoup(html_content, \"lxml\")\n",
    "            \n",
    "            for i in soup.find('span',{'class':['NLM_article-title' ,'hlFld-title']}):\n",
    "                # print(\"Title: \",i)\n",
    "                df.loc[row,'Title'] = i\n",
    "            \n",
    "            temp = soup.findAll('h2',{'class':['section-heading-2']} )\n",
    "            # print(\"Number of sections: \",len(temp))            \n",
    "            \n",
    "            count = 0\n",
    "            for i in range(len(temp)):\n",
    "\n",
    "                if temp[i].text == temp[i].find_next('p').text:\n",
    "        \n",
    "                    extract = \"section\" + str(count)\n",
    "                    extractData = temp[i].find_next('p').find_next('p').text\n",
    "                    df.loc[row,extract] = temp[i].text\n",
    "                    sectionData = \"sectionData\" + str(count)                    \n",
    "                    df.loc[row,sectionData] = extractData\n",
    "                    count = count + 1\n",
    "                else:\n",
    "                    extract = \"section\" + str(count)\n",
    "                    extractData = temp[i].find_next('p').text\n",
    "                    df.loc[row,extract] = temp[i].text\n",
    "                    sectionData = \"sectionData\" + str(count)\n",
    "                    df.loc[row,sectionData] = extractData\n",
    "                    count = count + 1\n",
    "            \n",
    "            \n",
    "            row = row + 1\n",
    "            print(\"row: \",row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.iloc[:, 0:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(pd.notnull(df), None)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd Desktop/MDSOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('KG-RCME-15099.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
